# üìä Data Preparation

## üìì Jupyter Notebooks Used

### üåä `flood_data_cleaning.ipynb`

This notebook cleans the raw data and generates synthetic data for any missing
 years. It aimed to extend an existing 10-year flood extent dataset (2015-2025)
  by generating 10 years of synthetic data (2005-2014) for two cities,
   Addis Ababa and Kampala.It does so in the following ways:

**Data Loading and Preparation**:  

- The original dataset, containing monthly flood water extent from 2015-2025, was
 loaded from a CSV file.

- Initial cleaning involved dropping irrelevant columns (system:index, source, .geo).

- The month column was converted from a string to a proper datetime format, which
 is essential for time series analysis.

- The data was sorted by city and date to ensure a correct chronological order.

**Pattern Extraction via Time Series Decomposition**:  

- To understand the underlying structure of the real data, a time series
 decomposition was performed for each city separately.

- This process separated the data into three key components:

  - Trend: The long-term change in flood extent (calculated via linear regression).
  - Seasonality: The repeating, predictable monthly pattern within a year.
  - Residuals: The random, irregular noise left over after removing the trend
   and seasonality.

- Any missing data points in the original dataset (as found in Kampala's data)
 were filled using a forward-fill method to ensure the decomposition could run smoothly.

**Synthetic Data Generation (2005-2014)**:  

- A new 10-year date range (Jan 2005 - Dec 2014) was created.

- For each month in this new range, a synthetic flood value was generated by
 combining the patterns extracted in the previous step:
  - Trend: The trend was linearly extrapolated backward from the starting point of
 the real data (January 2015).
  - Seasonality: The corresponding average seasonal value for that month was applied.
  - Noise: A random value was generated based on the statistical properties (mean
 and standard deviation) of the residuals from the real data.

- These three components were summed up. A final check ensured that no value was
 negative (any negative results were set to 0).

**Combining and Finalizing Datasets**:  

- The newly generated synthetic data and the original real data were merged into
 a single, continuous 20-year dataset (2005-2025).

- A new boolean column, is_synthetic, was added to clearly distinguish between
 the real and generated data points.

- The combined data was plotted to visually validate the results,
 confirming a seamless and realistic transition between the synthetic and real periods.

**Annual Aggregation and Export**:  

- Finally, the monthly data was aggregated into a total annual flood extent for
 each city. This smooths out seasonal fluctuations and makes it easier to
  analyze long-term, year-over-year trends.

This notebook creates and saves the cleaned
 [flood water extent](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/flood_synthetic_data.csv)
  data.

---

### üåÜ `urban_data_cleaning.ipynb`

This notebook cleans the raw urban area data and generates synthetic data for
 the missing years in the following steps:

**Data Import and Filtering**:  

- Installed openpyxl to read Excel files
- Loaded GHSL_excel.xlsx dataset
- Filtered dataset to keep only rows for Addis Ababa and Kampala.

**Year Selection and Data Reshaping**:  

- Selected urban area columns for years: 2000, 2005, 2010, 2015, 2020, 2025
- Melted the data into long format:  
  - Columns: Cities, Year, Urban_Area
  - Converted year strings to integers (e.g., "GH_BUS_TOT_2000" ‚Üí 2000).

**Synthetic Data Generation**:  

- Created all combinations of cities and years (2000-2024)
- Merged with existing data (leaving missing years as NaN)
- Marked synthetic points with is_synthetic flag
- Interpolation: Filled missing years using cubic spline interpolation
- Noise Addition: Added Gaussian noise (5% of std dev) to synthetic points.

**Output Validation**:  

- Displayed head/tail of final dataset
- Visualized results with line plot

This notebook creates and saves the cleaned
 [urban extent](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/Urban_synthetic_data.csv)
  data

---

### üåßÔ∏è `rainfall_data_cleaning.ipynb`

This notebook creates and saves the cleaned
 [rainfall](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/yearly_rainfall.csv)
  data.

---

### üîó `merged_data.ipynb`

This notebook merges all the three cleaned datasets.

This notebook creates and saves the cleaned [all merged data](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/merged_data_new2.csv).

---

### üåßÔ∏è `extract_chirps_rainfall_kampala_addis`

This notebook extracts raw monthly rainfall data for Kampala and Addis Ababa from
 Africa monthly rainfall data CHIRPS raster `(.tif)` files, using python
  libraries of rasterio and pandas.

It extracts the raw rainfall data by;

- Defining the co-ordinates of the respective cities, to be extracted from the
 `.tiff` files of Africa monthly.

- Defining the paths of the files, one containing the `.tiff` files, and the
 other being the destination file for the extracted raw rainfall data.

- Initializing a list that will hold all the extracted rainfall records being
 they are turned into a **DataFrame**.

- Creating a loop that runs through all the available `.tiff` files, one by
 one, in order.

- Processing only `.tif` files ignoring other file formats and extracting the
 dates from the file names.

- Opening the `.tif` files using the rasterio python library and then loops
 through the respective cities, also extracts the rainfall records of the given
  city coordinates.

- Converting the extracted rainfall records to a dataframe and then saving them.

This notebook creates and saves the raw [rainfall data](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/raw_data/Rainfall_Data.csv)
