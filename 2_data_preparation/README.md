# üìä Data Preparation

## üìì Jupyter Notebooks Used

### üåä [`flood_data_cleaning.ipynb`](<https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/2_data_preparation/cleaning_scripts/flood_data_cleaning.ipynb>)

This notebook cleans the raw data and generates synthetic data for any missing
 years. It aimed to extend an existing 10-year flood extent dataset (2015-2025)
  by generating 10 years of synthetic data (2005-2014) for two cities, Addis
   Ababa and Kampala.It does so in the following ways:

**Data Loading and Preparation**:  

- The original dataset, containing monthly flood water extent from 2015-2025, was
 loaded from a CSV file.

- Initial cleaning involved dropping irrelevant columns (system:index, source, .geo).

- The month column was converted from a string to a proper datetime format, which
 is essential for time series analysis.

- The data was sorted by city and date to ensure a correct chronological order.

**Pattern Extraction via Time Series Decomposition**:  

- To understand the underlying structure of the real data, a time series
 decomposition was performed for each city separately.

- This process separated the data into three key components:

  - Trend: The long-term change in flood extent (calculated via linear regression).
  - Seasonality: The repeating, predictable monthly pattern within a year.
  - Residuals: The random, irregular noise left over after removing the trend
   and seasonality.

- Any missing data points in the original dataset (as found in Kampala's data)
 were filled using a forward-fill method to ensure the decomposition could run smoothly.

**Synthetic Data Generation (2005-2014)**:  

- A new 10-year date range (Jan 2005 - Dec 2014) was created.

- For each month in this new range, a synthetic flood value was generated by
 combining the patterns extracted in the previous step:
  - Trend: The trend was linearly extrapolated backward from the starting point of
 the real data (January 2015).
  - Seasonality: The corresponding average seasonal value for that month was applied.
  - Noise: A random value was generated based on the statistical properties (mean
 and standard deviation) of the residuals from the real data.

- These three components were summed up. A final check ensured that no value was
 negative (any negative results were set to 0).

**Combining and Finalizing Datasets**:  

- The newly generated synthetic data and the original real data were merged into
 a single, continuous 20-year dataset (2005-2025).

- A new boolean column, is_synthetic, was added to clearly distinguish between
 the real and generated data points.

- The combined data was plotted to visually validate the results,
 confirming a seamless and realistic transition between the synthetic and real periods.

**Annual Aggregation and Export**:  

- Finally, the monthly data was aggregated into a total annual flood extent for
 each city. This smooths out seasonal fluctuations and makes it easier to
  analyze long-term, year-over-year trends.

This notebook creates and saves the cleaned
 [flood water extent](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/flood_synthetic_data.csv)
  data.

---

### üåÜ [`urban_data_cleaning.ipynb`](<https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/2_data_preparation/cleaning_scripts/urban_data_cleaning.ipynb>)

This notebook cleans the raw urban area data and generates synthetic data for
 the missing years in the following steps:

**Data Import and Filtering**:  

- Installed openpyxl to read Excel files
- Loaded GHSL_excel.xlsx dataset
- Filtered dataset to keep only rows for Addis Ababa and Kampala.

**Year Selection and Data Reshaping**:  

- Selected urban area columns for years: 2000, 2005, 2010, 2015, 2020, 2025
- Melted the data into long format:  
  - Columns: Cities, Year, Urban_Area
  - Converted year strings to integers (e.g., "GH_BUS_TOT_2000" ‚Üí 2000).

**Synthetic Data Generation**:  

- Created all combinations of cities and years (2000-2024)
- Merged with existing data (leaving missing years as NaN)
- Marked synthetic points with is_synthetic flag
- Interpolation: Filled missing years using cubic spline interpolation
- Noise Addition: Added Gaussian noise (5% of std dev) to synthetic points.

**Output Validation**:  

- Displayed head/tail of final dataset
- Visualized results with line plot

This notebook creates and saves the cleaned
 [urban extent](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/Urban_synthetic_data.csv)
  data

---

### üåßÔ∏è [`rainfall_data_cleaning.ipynb`](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/2_data_preparation/cleaning_scripts/rainfall_data_cleaning.ipynb)

This notebook processes the raw Rainfall dataset in the following ways:

1. **Import Library**

   - `pandas` is imported for data manipulation.

2. **Load Dataset**

   - The CSV file is read directly from the GitHub repository using `pd.read_csv()`.

3. **Convert Dates**

   - The `"Date"` column is converted from string to a datetime format using `pd.to_datetime()`.

4. **Extract Year**

   - A new `"Year"` column is created by extracting the year component from the `"Date"`.

5. **Aggregate Rainfall Annually**

   - Data is grouped by `"City"` and `"Year"`, then the total annual rainfall
   (`"Rainfall_mm"`) is summed using `groupby()` and `sum()`.

6. **Export Result**

   - The resulting annual rainfall dataset is saved locally as `yearly_rainfall.csv`.

This notebook creates and saves the cleaned
 [rainfall](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/yearly_rainfall.csv)
  data.

---

### üîó [`merged_data.ipynb`](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/2_data_preparation/cleaning_scripts/merged_data.ipynb)

This notebook merges all the three cleaned datasets, in the following steps:

1. **Load Data**

   - Reads in three CSV files (`rainfall.csv`, `flood.csv`, `urban.csv`) into
   separate DataFrames.

2. **Synthetic Data Labeling**

   - Adds a boolean column `is_synthetic_rainfall` to the rainfall DataFrame
   and sets all values to `False`.

3. **Standardize Column Names**

   - Renames columns in `flood` and `urban` to ensure consistency across datasets:

     - `"year"` ‚Üí `"Year"`
     - `"city"` or `"Cities"` ‚Üí `"City"`
     - `"is_synthetic"` ‚Üí respective `is_synthetic_flood` or `is_synthetic_urban`

4. **Merge Datasets**

   - Merges all three datasets (`rainfall`, `flood`, `urban`) on the common
   columns `["City", "Year"]`.

5. **Reorder Columns**

   - Swaps the position of `Total Flood Extent (km^2)` and `is_synthetic_flood`
   for improved readability.

6. **Export Final Merged Dataset**

   - Saves the combined dataset as `merged_data.csv`.

This notebook creates and saves the cleaned [all merged data](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/clean_data/merged_data_new2.csv).

---

### üåßÔ∏è [`extract_chirps_rainfall_kampala_addis`](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/2_data_preparation/cleaning_scripts/extract_chirps_rainfall_kampala_addis.ipynb)

This notebook extracts raw monthly rainfall data for Kampala and Addis Ababa from
 Africa monthly rainfall data CHIRPS raster `(.tif)` files, using python
  libraries of rasterio and pandas.

It extracts the raw rainfall data by;

- Defining the co-ordinates of the respective cities, to be extracted from the
 `.tiff` files of Africa monthly.

- Defining the paths of the files, one containing the `.tiff` files, and the
 other being the destination file for the extracted raw rainfall data.

- Initializing a list that will hold all the extracted rainfall records being
 they are turned into a **DataFrame**.

- Creating a loop that runs through all the available `.tiff` files, one by
 one, in order.

- Processing only `.tif` files ignoring other file formats and extracting the
 dates from the file names.

- Opening the `.tif` files using the rasterio python library and then loops
 through the respective cities, also extracts the rainfall records of the given
  city coordinates.

- Converting the extracted rainfall records to a dataframe and then saving them.

This notebook creates and saves the raw [rainfall data](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-01-repo/blob/main/1_datasets/raw_data/Rainfall_Data.csv)
